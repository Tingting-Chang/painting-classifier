{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import fns_models as fns\n",
    "\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, train_labels, test, test_labels = fns.get_top_author(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================\n",
    "\n",
    "# stacking \n",
    "\n",
    "[stacking](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)\n",
    "\n",
    "============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train\n",
    "y = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stacking he class-probabilities of the first-level classifiers can be used to train the meta-classifier (2nd-level classifier)\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stacking using GridSearch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "# Initializing models\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "          'randomforestclassifier__n_estimators': [10, 50],\n",
    "          'meta-logisticregression__C': [0.1, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=sclf, \n",
    "                    param_grid=params, \n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "grid.fit(X, y)\n",
    "\n",
    "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    print(\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "print('Best parameters: %s' % grid.best_params_)\n",
    "print('Accuracy: %.2f' % grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacking using baysian optimization\n",
    "# stacking using GridSearch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "def stack_fn(sclf, params):\n",
    "    \n",
    "\n",
    "# Initializing models\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "          'randomforestclassifier__n_estimators': [10, 50],\n",
    "          'meta-logisticregression__C': [0.1, 10.0]}\n",
    "\n",
    "# grid = GridSearchCV(estimator=sclf, \n",
    "#                     param_grid=params, \n",
    "#                     cv=5,\n",
    "#                     refit=True)\n",
    "\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "random_state = 2017\n",
    "    \n",
    "sBO = BayesianOptimization(sclf, params)\n",
    "sBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print \"Bayesian Optimization Best Score: %d\" % sBO.res['max']['max_val']\n",
    "\n",
    "print \"Bayesian Optimization Best Parameters: %s\" % str(sBO.res['max']['max_params'])\n",
    "\n",
    "print (lrBO.res['max'])\n",
    "\n",
    "fns.plot_bo(lr_fnc, lrBO)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacking LG + RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================\n",
    "\n",
    "# stacking xgboost\n",
    "\n",
    "[xgboost stacking](https://github.com/AntonUBC/kaggle_flavours_of_physics)\n",
    "\n",
    "============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "from hep_ml.losses import BinFlatnessLossFunction\n",
    "\n",
    "\n",
    "\n",
    "def Model1(train, test, train_labels):\n",
    "    \n",
    "# Model 1 is an ensemble of XGBoost, Random Forest and Uniform Gradient Boosting Classifiers\n",
    "# which are trained using the stacked data    \n",
    "\n",
    "    model = 1    # set the model number for feature engineering\n",
    "    n_folds = 3 # set the number of folders for generating meta-features\n",
    "    n_stack = 15  # number of models used for stacking\n",
    "    \n",
    "    \n",
    "    # Initialize models for stacking\n",
    "        \n",
    "    clf1=KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf2=KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf3=KNeighborsClassifier(n_neighbors=20, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)  \n",
    "                          \n",
    "    clf4=KNeighborsClassifier(n_neighbors=40, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf5=KNeighborsClassifier(n_neighbors=80, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None) \n",
    "\n",
    "    clf6=KNeighborsClassifier(n_neighbors=160, weights='uniform', algorithm='auto', leaf_size=30,  \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "\n",
    "    clf7=KNeighborsClassifier(n_neighbors=320, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)                          \n",
    "                          \n",
    "    clf8=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=5.0, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight=None, random_state=101, solver='lbfgs', \n",
    "                            max_iter=200, multi_class='ovr', verbose=0) \n",
    "                        \n",
    "    clf9=GaussianNB()\n",
    "                 \n",
    "    clf10=SVC(C=5.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.008, shrinking=True, probability=True, \n",
    "              tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=101)\n",
    "               \n",
    "    clf11=RandomForestClassifier(n_estimators=250, criterion='gini', max_depth=6, min_samples_split=2, \n",
    "                            min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=0.7, \n",
    "                            max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=2,\n",
    "                            random_state=101, verbose=0, warm_start=False, class_weight=None) \n",
    "                            \n",
    "    clf12=ExtraTreesClassifier(n_estimators=250, criterion='gini', max_depth=6, min_samples_split=2,\n",
    "                     min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=0.7,\n",
    "                     max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=2, \n",
    "                     random_state=101, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "    clf13=GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=450, subsample=0.7, \n",
    "                                min_samples_split=2, min_samples_leaf=5, min_weight_fraction_leaf=0.0,\n",
    "                                max_depth=6, init=None, random_state=101, max_features=None, verbose=0,\n",
    "                                max_leaf_nodes=None, warm_start=False)\n",
    "\n",
    "    clf14=models.XGBoostClassifier(nthread=2, eta=.2, gamma=0, max_depth=6, min_child_weight=3, max_delta_step=0,\n",
    "                         subsample=0.7, colsample_bytree=0.7, silent =1, seed=101,\n",
    "                         l2_reg=1, l1_reg=0, n_estimators=450)\n",
    "                         \n",
    "                               \n",
    "    clfs = [clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10, clf11, clf12, clf13, clf14]    \n",
    "        \n",
    "    # Construct stacked datasets\n",
    "#     train_blend, test_blend, train_probs, test_probs = StackModels(train[features], test[features], \n",
    "#                                                                          train.signal.values, clfs, n_folds)                                                                                      \n",
    "    \n",
    "    train_blend, test_blend, train_probs, test_probs = StackModels(train, test, train_labels, clfs, n_folds)                                                                                      \n",
    "                                                                              \n",
    "    # Construct data for uniform boosting\n",
    "    columns = ['p%s ' % (i) for i in range(0, n_stack)]\n",
    "    meta_train = pd.DataFrame({columns[i]: train_probs[:, i] for i in range(0, n_stack)})\n",
    "    meta_test = pd.DataFrame({columns[i]: test_probs[:, i] for i in range(0, n_stack)})\n",
    "    train_ugb = pd.concat([train, meta_train], axis=1)\n",
    "    test_ugb = pd.concat([test, meta_test], axis=1)\n",
    "    features_ugb = features + columns               # features used for UGB training (original features + meta-features)\n",
    "\n",
    "    # Initialize models for ensemble\n",
    "    loss = BinFlatnessLossFunction(['mass'], n_bins=20, power=1, fl_coefficient=3, uniform_label=0)\n",
    "                                   \n",
    "    clf_ugb = UGradientBoostingClassifier(loss=loss, n_estimators=275, max_depth=11, min_samples_leaf=3, \n",
    "                            learning_rate=0.03, train_features=features_ugb, subsample=0.85, random_state=101)  \n",
    "                            \n",
    "    clf_xgb = models.XGBoostClassifier(nthread=6, eta=.0225, gamma=1.225, max_depth=11, min_child_weight=10, \n",
    "                                max_delta_step=0, subsample=0.8, colsample_bytree=0.3,  \n",
    "                                silent =1, seed=101, l2_reg=1, l1_reg=0, n_estimators=1100)\n",
    "                                \n",
    "    clf_rf = RandomForestClassifier(n_estimators=375, criterion='gini', max_depth=10, min_samples_split=6, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=0.6, \n",
    "                                max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=4,\n",
    "                                random_state=101, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "    # Train models\n",
    "    print(\"Training a Uniform Gradient Boosting model\")     \n",
    "    clf_ugb.fit(train_ugb[features_ugb + ['mass']], train_ugb['signal'])   \n",
    "    preds_ugb = clf_ugb.predict_proba(test_ugb)[:,1]\n",
    "    \n",
    "    print(\"Training a XGBoost model\")     \n",
    "    clf_xgb.fit(train_blend, train['signal'])\n",
    "    preds_xgb = clf_xgb.predict_proba(test_blend)\n",
    "        \n",
    "    print(\"Training a Random Forest model\") \n",
    "    clf_rf.fit(train_blend, train['signal'])\n",
    "    preds_rf = clf_rf.predict_proba(test_blend)[:,1]\n",
    "        \n",
    "    # Compute ensemble predictions\n",
    "    preds = 0.3*(preds_xgb**(0.65))*(preds_rf**(0.35)) + 0.7*preds_ugb\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def Model2():\n",
    "    \n",
    "# Model 2 is a single XGBoost classifier \"undertrained\" to reduce correlation with tau-mass       \n",
    "\n",
    "    model = 2    # set the model number for feature engineering\n",
    "                                                         \n",
    "    train, test, features = utils.LoadData(model)    # load data\n",
    "    \n",
    "    # Initialize a XGBoost model\n",
    "    clf_xgb = models.XGBoostClassifier(nthread=6, eta=0.75, gamma=1.125, max_depth=8, min_child_weight=5, \n",
    "                                max_delta_step=0, subsample=0.7, colsample_bytree=0.7, silent=1, seed=1, \n",
    "                                l2_reg=1, l1_reg=0, n_estimators=50)                                \n",
    "                              \n",
    "    # Train a XGBoost model                                                                   \n",
    "    print(\"Training a XGBoost model\")  \n",
    "    clf_xgb.fit(train, train_labels)\n",
    "   \n",
    "    # Calculate predictions\n",
    "    preds = clf_xgb.predict_proba(test)\n",
    "    return preds\n",
    "\n",
    "print(\"Training Model1\")    \n",
    "preds_model1 = Model1()         # compute predictions of Model1\n",
    "\n",
    "print(\"Training Model2\")\n",
    "preds_model2 = Model2()         # compute predictions of Model2\n",
    "\n",
    "# compute final predictions for submission  \n",
    "preds_ensemble = (preds_model1**0.585) * (preds_model2**0.415)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script contains functions used for data loading, feature engineering, and saving predictions\n",
    "# It also contains a stacking function, used to obtain meta-features for the second stage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from flavours_utils import paths\n",
    "\n",
    "\n",
    "\n",
    "y = train_labels\n",
    "\n",
    "def StackModels(train, test, y, clfs, n_folds): # train data (pd data frame), test data (pd date frame), Target data,\n",
    "                                                # list of models to stack, number of folders\n",
    "\n",
    "# StackModels() performs Stacked Aggregation on data: it uses n different classifiers to get out-of-fold \n",
    "# predicted probabilities of signal for train data. It uses the whole training dataset to obtain predictions for test.\n",
    "# This procedure adds n meta-features to both train and test data (where n is number of models to stack).\n",
    "\n",
    "    print(\"Generating Meta-features\")\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "    training = train.as_matrix()\n",
    "    testing = test.as_matrix()\n",
    "    scaler = StandardScaler().fit(training)\n",
    "    train_all = scaler.transform(training)\n",
    "    test_all = scaler.transform(testing)\n",
    "    blend_train = np.zeros((training.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((testing.shape[0], len(clfs)))   # Number of testing data x Number of classifiers\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        \n",
    "        print ('Training classifier [%s]' % (j))\n",
    "        for i, (tr_index, cv_index) in enumerate(skf):\n",
    "            \n",
    "            print ('stacking Fold [%s] of train data' % (i))\n",
    "            \n",
    "            # This is the training and validation set (train on 2 folders, predict on a 3d folder)\n",
    "            X_train = training[tr_index]\n",
    "            Y_train = y[tr_index]\n",
    "            X_cv = training[cv_index]\n",
    "            scaler=StandardScaler().fit(X_train)\n",
    "            X_train=scaler.transform(X_train)\n",
    "            X_cv=scaler.transform(X_cv)\n",
    "                                  \n",
    "            clf.fit(X_train, Y_train)\n",
    "            pred = clf.predict_proba(X_cv)\n",
    "            \n",
    "            if pred.ndim==1:  # XGBoost produces ONLY probabilities of success as opposed to sklearn models\n",
    "                 \n",
    "                 blend_train[cv_index, j] = pred\n",
    "                 \n",
    "            else:\n",
    "                \n",
    "                blend_train[cv_index, j] = pred[:, 1]\n",
    "        \n",
    "        print('stacking test data')        \n",
    "        clf.fit(train_all, y)\n",
    "        pred = clf.predict_proba(test_all)\n",
    "        \n",
    "        if pred.ndim==1 :      # XGBoost produces ONLY probabilities of success as opposed to sklearn models\n",
    "        \n",
    "           blend_test[:, j] = pred\n",
    "           \n",
    "        else:\n",
    "            \n",
    "           blend_test[:, j] = pred[:, 1]\n",
    "\n",
    "    X_train_blend=np.concatenate((training, blend_train), axis=1)\n",
    "    X_test_blend=np.concatenate((testing, blend_test), axis=1)\n",
    "    return X_train_blend, X_test_blend, blend_train, blend_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
