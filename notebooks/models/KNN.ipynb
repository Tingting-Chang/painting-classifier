{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "======================================================================================================\n",
    "\n",
    "# KNN & PCA\n",
    "\n",
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athenaeum_authors.csv\n",
      "athenaeum_authors_preview.csv\n",
      "athenaeum_painting_filtered.csv\n",
      "athenaeum_painting_movement.csv\n",
      "athenaeum_painting_movement_test.csv\n",
      "athenaeum_painting_movement_train.csv\n",
      "athenaeum_paintings.csv\n",
      "athenaeum_paintings_sizes.csv\n",
      "color_hist_kmeans_206552.csv\n",
      "color_histograms.csv\n",
      "color_hist_size_206552.csv\n",
      "complete_data.csv\n",
      "extra_tree_com.csv\n",
      "grad_boost_com.csv\n",
      "images\n",
      "images_athenaeum\n",
      "images_sizes_2325.csv\n",
      "model_accuracy.csv\n",
      "nbc_com.csv\n",
      "net_predicted.csv\n",
      "painter_info_clean.csv\n",
      "painting_info_clean.csv\n",
      "pca20_kmeans_test.csv\n",
      "pca20_kmeans_train.csv\n",
      "resized_200\n",
      "rf_com.csv\n",
      "test_author200.csv\n",
      "test_data.csv\n",
      "test_hist_author_knn.csv\n",
      "test_hist_author_rf.csv\n",
      "train_author200.csv\n",
      "train_data.csv\n",
      "train_hist_author_knn.csv\n",
      "train_hist_author_rf.csv\n",
      "xgb_com.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "# import sklearn.grid_search\n",
    "\n",
    "from src import fns_models as fns\n",
    "\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The size of train histogram for Random Forest(49890, 35)\n",
      "[INFO] The size of test histogram for Random Forest(12473, 35)\n",
      "24      1369\n",
      "1793    1338\n",
      "368     1335\n",
      "Name: author_id, dtype: int64\n",
      "[trian above] ==================================================[test below]\n",
      "24      342\n",
      "1793    335\n",
      "368     334\n",
      "Name: author_id, dtype: int64\n",
      "(4042,)\n",
      "(4042, 35)\n"
     ]
    }
   ],
   "source": [
    "train, train_labels, test, test_labels = fns.get_top_author(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hist_01</th>\n",
       "      <th>hist_02</th>\n",
       "      <th>hist_03</th>\n",
       "      <th>hist_04</th>\n",
       "      <th>hist_05</th>\n",
       "      <th>hist_06</th>\n",
       "      <th>hist_07</th>\n",
       "      <th>hist_08</th>\n",
       "      <th>hist_09</th>\n",
       "      <th>hist_10</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_22</th>\n",
       "      <th>hist_23</th>\n",
       "      <th>hist_24</th>\n",
       "      <th>hist_25</th>\n",
       "      <th>hist_26</th>\n",
       "      <th>hist_27</th>\n",
       "      <th>hist_28</th>\n",
       "      <th>hist_29</th>\n",
       "      <th>hist_30</th>\n",
       "      <th>height_width_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.075736</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064973</td>\n",
       "      <td>0.155495</td>\n",
       "      <td>0.198439</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.106906</td>\n",
       "      <td>0.188239</td>\n",
       "      <td>0.087873</td>\n",
       "      <td>0.033008</td>\n",
       "      <td>0.583974</td>\n",
       "      <td>1.984496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.113157</td>\n",
       "      <td>0.100789</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.024289</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120838</td>\n",
       "      <td>0.122384</td>\n",
       "      <td>0.240406</td>\n",
       "      <td>0.066717</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.036815</td>\n",
       "      <td>0.124549</td>\n",
       "      <td>0.168427</td>\n",
       "      <td>0.663395</td>\n",
       "      <td>1.565749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.039886</td>\n",
       "      <td>0.167470</td>\n",
       "      <td>0.076083</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484468</td>\n",
       "      <td>0.307293</td>\n",
       "      <td>0.022210</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.011258</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>0.110167</td>\n",
       "      <td>0.263844</td>\n",
       "      <td>0.576894</td>\n",
       "      <td>1.412414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hist_01   hist_02   hist_03   hist_04   hist_05   hist_06   hist_07  \\\n",
       "177  0.075736  0.001017  0.000195  0.000061  0.000091  0.000167  0.000124   \n",
       "381  0.113157  0.100789  0.008086  0.001295  0.001914  0.004696  0.004417   \n",
       "383  0.039886  0.167470  0.076083  0.001664  0.000785  0.000676  0.000319   \n",
       "\n",
       "      hist_08   hist_09   hist_10         ...           hist_22   hist_23  \\\n",
       "177  0.000265  0.000814  0.010185         ...          0.064973  0.155495   \n",
       "381  0.011081  0.024289  0.064011         ...          0.120838  0.122384   \n",
       "383  0.000550  0.000656  0.002208         ...          0.484468  0.307293   \n",
       "\n",
       "      hist_24   hist_25   hist_26   hist_27   hist_28   hist_29   hist_30  \\\n",
       "177  0.198439  0.022097  0.106906  0.188239  0.087873  0.033008  0.583974   \n",
       "381  0.240406  0.066717  0.006814  0.036815  0.124549  0.168427  0.663395   \n",
       "383  0.022210  0.002730  0.011258  0.037837  0.110167  0.263844  0.576894   \n",
       "\n",
       "     height_width_ratio  \n",
       "177            1.984496  \n",
       "381            1.565749  \n",
       "383            1.412414  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "======================================================================================================\n",
    "\n",
    "# KNN \n",
    "\n",
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# random.seed(2017)\n",
    "\n",
    "# X_trian, X_val, y_train, y_val = train_test_split(train, train_labels, test_size = 0.5, random_state=0)\n",
    "\n",
    "# print \"Length of Validation Set:\" + str(len(y_val))\n",
    "# print \"Length of Train Set:\" + str(len(y_train))\n",
    "# print \"Ratio: \" + str(len(y_val) / float(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "int(math.ceil(math.sqrt(len(train))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'n_neighbors': [5, 10, 15, 20, 30, 40, 50, 64, 70, 80, 90, 100, 125, 200, 250], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'p': [1, 2, 9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the KNN model\n",
    "print \"Training KNN Model...\"\n",
    "\n",
    "# minkowski_distance\n",
    "k = [5, 10, 15, 20, 30, 40, 50, 64, 70, 80, 90, 100, 125, 200, 250]\n",
    "#k = [5, 20]\n",
    "parameters = {'n_neighbors': k, 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1, 2, 9]}\n",
    "knn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(knn, parameters, cv=6, n_jobs=4)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Model on testing set:0.707220573689\n",
      "Accuracy of the Model on training set:0.705838693716\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train, train_labels)\n",
    "# accuracy for the test set\n",
    "print \"Accuracy of the Model on testing set:\" + str(clf.score(test,test_labels))\n",
    "\n",
    "# accuracy for the train set\n",
    "print \"Accuracy of the Model on training set:\" + str(clf.score(train,train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "()\n",
      "{'n_neighbors': 30, 'algorithm': 'auto', 'p': 1}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.644 (+/-0.040) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 1}\n",
      "0.629 (+/-0.054) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 2}\n",
      "0.602 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 9}\n",
      "0.667 (+/-0.042) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 1}\n",
      "0.648 (+/-0.056) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 2}\n",
      "0.624 (+/-0.035) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 9}\n",
      "0.672 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 1}\n",
      "0.652 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 2}\n",
      "0.633 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 9}\n",
      "0.673 (+/-0.039) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 1}\n",
      "0.657 (+/-0.048) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 2}\n",
      "0.626 (+/-0.027) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 9}\n",
      "0.675 (+/-0.041) for {'n_neighbors': 30, 'algorithm': 'auto', 'p': 1}\n",
      "0.654 (+/-0.042) for {'n_neighbors': 30, 'algorithm': 'auto', 'p': 2}\n",
      "0.624 (+/-0.045) for {'n_neighbors': 30, 'algorithm': 'auto', 'p': 9}\n",
      "0.667 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'auto', 'p': 1}\n",
      "0.650 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'auto', 'p': 2}\n",
      "0.625 (+/-0.045) for {'n_neighbors': 40, 'algorithm': 'auto', 'p': 9}\n",
      "0.669 (+/-0.049) for {'n_neighbors': 50, 'algorithm': 'auto', 'p': 1}\n",
      "0.645 (+/-0.046) for {'n_neighbors': 50, 'algorithm': 'auto', 'p': 2}\n",
      "0.624 (+/-0.047) for {'n_neighbors': 50, 'algorithm': 'auto', 'p': 9}\n",
      "0.664 (+/-0.043) for {'n_neighbors': 64, 'algorithm': 'auto', 'p': 1}\n",
      "0.645 (+/-0.035) for {'n_neighbors': 64, 'algorithm': 'auto', 'p': 2}\n",
      "0.621 (+/-0.031) for {'n_neighbors': 64, 'algorithm': 'auto', 'p': 9}\n",
      "0.665 (+/-0.042) for {'n_neighbors': 70, 'algorithm': 'auto', 'p': 1}\n",
      "0.646 (+/-0.043) for {'n_neighbors': 70, 'algorithm': 'auto', 'p': 2}\n",
      "0.618 (+/-0.036) for {'n_neighbors': 70, 'algorithm': 'auto', 'p': 9}\n",
      "0.661 (+/-0.036) for {'n_neighbors': 80, 'algorithm': 'auto', 'p': 1}\n",
      "0.644 (+/-0.038) for {'n_neighbors': 80, 'algorithm': 'auto', 'p': 2}\n",
      "0.621 (+/-0.041) for {'n_neighbors': 80, 'algorithm': 'auto', 'p': 9}\n",
      "0.662 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'auto', 'p': 1}\n",
      "0.643 (+/-0.042) for {'n_neighbors': 90, 'algorithm': 'auto', 'p': 2}\n",
      "0.616 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'auto', 'p': 9}\n",
      "0.662 (+/-0.038) for {'n_neighbors': 100, 'algorithm': 'auto', 'p': 1}\n",
      "0.639 (+/-0.043) for {'n_neighbors': 100, 'algorithm': 'auto', 'p': 2}\n",
      "0.610 (+/-0.039) for {'n_neighbors': 100, 'algorithm': 'auto', 'p': 9}\n",
      "0.661 (+/-0.032) for {'n_neighbors': 125, 'algorithm': 'auto', 'p': 1}\n",
      "0.634 (+/-0.028) for {'n_neighbors': 125, 'algorithm': 'auto', 'p': 2}\n",
      "0.606 (+/-0.045) for {'n_neighbors': 125, 'algorithm': 'auto', 'p': 9}\n",
      "0.651 (+/-0.032) for {'n_neighbors': 200, 'algorithm': 'auto', 'p': 1}\n",
      "0.619 (+/-0.034) for {'n_neighbors': 200, 'algorithm': 'auto', 'p': 2}\n",
      "0.595 (+/-0.050) for {'n_neighbors': 200, 'algorithm': 'auto', 'p': 9}\n",
      "0.642 (+/-0.035) for {'n_neighbors': 250, 'algorithm': 'auto', 'p': 1}\n",
      "0.615 (+/-0.024) for {'n_neighbors': 250, 'algorithm': 'auto', 'p': 2}\n",
      "0.586 (+/-0.053) for {'n_neighbors': 250, 'algorithm': 'auto', 'p': 9}\n",
      "0.644 (+/-0.040) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.629 (+/-0.054) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.602 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.667 (+/-0.042) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.648 (+/-0.056) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.624 (+/-0.035) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.672 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.652 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.633 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.673 (+/-0.039) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.657 (+/-0.048) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.626 (+/-0.027) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.675 (+/-0.041) for {'n_neighbors': 30, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.654 (+/-0.042) for {'n_neighbors': 30, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.624 (+/-0.045) for {'n_neighbors': 30, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.667 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.650 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.625 (+/-0.045) for {'n_neighbors': 40, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.669 (+/-0.049) for {'n_neighbors': 50, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.645 (+/-0.046) for {'n_neighbors': 50, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.624 (+/-0.047) for {'n_neighbors': 50, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.664 (+/-0.043) for {'n_neighbors': 64, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.645 (+/-0.035) for {'n_neighbors': 64, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.621 (+/-0.031) for {'n_neighbors': 64, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.665 (+/-0.042) for {'n_neighbors': 70, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.646 (+/-0.043) for {'n_neighbors': 70, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.618 (+/-0.036) for {'n_neighbors': 70, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.661 (+/-0.036) for {'n_neighbors': 80, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.644 (+/-0.038) for {'n_neighbors': 80, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.621 (+/-0.041) for {'n_neighbors': 80, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.662 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.643 (+/-0.042) for {'n_neighbors': 90, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.616 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.662 (+/-0.038) for {'n_neighbors': 100, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.639 (+/-0.043) for {'n_neighbors': 100, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.610 (+/-0.039) for {'n_neighbors': 100, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.661 (+/-0.032) for {'n_neighbors': 125, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.634 (+/-0.028) for {'n_neighbors': 125, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.606 (+/-0.045) for {'n_neighbors': 125, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.651 (+/-0.032) for {'n_neighbors': 200, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.619 (+/-0.034) for {'n_neighbors': 200, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.595 (+/-0.050) for {'n_neighbors': 200, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.642 (+/-0.035) for {'n_neighbors': 250, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.615 (+/-0.024) for {'n_neighbors': 250, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.586 (+/-0.053) for {'n_neighbors': 250, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.644 (+/-0.040) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.629 (+/-0.054) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.602 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.667 (+/-0.042) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.648 (+/-0.056) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.624 (+/-0.035) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.672 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.652 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.633 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.673 (+/-0.039) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.657 (+/-0.048) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.626 (+/-0.027) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.675 (+/-0.041) for {'n_neighbors': 30, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.654 (+/-0.042) for {'n_neighbors': 30, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.624 (+/-0.045) for {'n_neighbors': 30, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.667 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.650 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.625 (+/-0.045) for {'n_neighbors': 40, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.669 (+/-0.049) for {'n_neighbors': 50, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.645 (+/-0.046) for {'n_neighbors': 50, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.624 (+/-0.047) for {'n_neighbors': 50, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.664 (+/-0.043) for {'n_neighbors': 64, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.645 (+/-0.035) for {'n_neighbors': 64, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.621 (+/-0.031) for {'n_neighbors': 64, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.665 (+/-0.042) for {'n_neighbors': 70, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.646 (+/-0.043) for {'n_neighbors': 70, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.618 (+/-0.036) for {'n_neighbors': 70, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.661 (+/-0.036) for {'n_neighbors': 80, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.644 (+/-0.038) for {'n_neighbors': 80, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.621 (+/-0.041) for {'n_neighbors': 80, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.662 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.643 (+/-0.042) for {'n_neighbors': 90, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.616 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.662 (+/-0.038) for {'n_neighbors': 100, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.639 (+/-0.043) for {'n_neighbors': 100, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.610 (+/-0.039) for {'n_neighbors': 100, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.661 (+/-0.032) for {'n_neighbors': 125, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.634 (+/-0.028) for {'n_neighbors': 125, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.606 (+/-0.045) for {'n_neighbors': 125, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.651 (+/-0.032) for {'n_neighbors': 200, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.619 (+/-0.034) for {'n_neighbors': 200, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.595 (+/-0.050) for {'n_neighbors': 200, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.642 (+/-0.035) for {'n_neighbors': 250, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.615 (+/-0.024) for {'n_neighbors': 250, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.586 (+/-0.053) for {'n_neighbors': 250, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.644 (+/-0.040) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 1}\n",
      "0.629 (+/-0.054) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 2}\n",
      "0.602 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 9}\n",
      "0.667 (+/-0.042) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 1}\n",
      "0.648 (+/-0.056) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 2}\n",
      "0.624 (+/-0.035) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 9}\n",
      "0.672 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 1}\n",
      "0.652 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 2}\n",
      "0.633 (+/-0.037) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 9}\n",
      "0.673 (+/-0.039) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 1}\n",
      "0.657 (+/-0.048) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 2}\n",
      "0.626 (+/-0.027) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 9}\n",
      "0.675 (+/-0.041) for {'n_neighbors': 30, 'algorithm': 'brute', 'p': 1}\n",
      "0.654 (+/-0.042) for {'n_neighbors': 30, 'algorithm': 'brute', 'p': 2}\n",
      "0.624 (+/-0.045) for {'n_neighbors': 30, 'algorithm': 'brute', 'p': 9}\n",
      "0.667 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'brute', 'p': 1}\n",
      "0.650 (+/-0.049) for {'n_neighbors': 40, 'algorithm': 'brute', 'p': 2}\n",
      "0.625 (+/-0.045) for {'n_neighbors': 40, 'algorithm': 'brute', 'p': 9}\n",
      "0.669 (+/-0.049) for {'n_neighbors': 50, 'algorithm': 'brute', 'p': 1}\n",
      "0.645 (+/-0.046) for {'n_neighbors': 50, 'algorithm': 'brute', 'p': 2}\n",
      "0.624 (+/-0.047) for {'n_neighbors': 50, 'algorithm': 'brute', 'p': 9}\n",
      "0.664 (+/-0.043) for {'n_neighbors': 64, 'algorithm': 'brute', 'p': 1}\n",
      "0.645 (+/-0.035) for {'n_neighbors': 64, 'algorithm': 'brute', 'p': 2}\n",
      "0.621 (+/-0.031) for {'n_neighbors': 64, 'algorithm': 'brute', 'p': 9}\n",
      "0.665 (+/-0.042) for {'n_neighbors': 70, 'algorithm': 'brute', 'p': 1}\n",
      "0.646 (+/-0.043) for {'n_neighbors': 70, 'algorithm': 'brute', 'p': 2}\n",
      "0.618 (+/-0.036) for {'n_neighbors': 70, 'algorithm': 'brute', 'p': 9}\n",
      "0.661 (+/-0.036) for {'n_neighbors': 80, 'algorithm': 'brute', 'p': 1}\n",
      "0.644 (+/-0.038) for {'n_neighbors': 80, 'algorithm': 'brute', 'p': 2}\n",
      "0.621 (+/-0.041) for {'n_neighbors': 80, 'algorithm': 'brute', 'p': 9}\n",
      "0.662 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'brute', 'p': 1}\n",
      "0.643 (+/-0.042) for {'n_neighbors': 90, 'algorithm': 'brute', 'p': 2}\n",
      "0.616 (+/-0.036) for {'n_neighbors': 90, 'algorithm': 'brute', 'p': 9}\n",
      "0.662 (+/-0.038) for {'n_neighbors': 100, 'algorithm': 'brute', 'p': 1}\n",
      "0.639 (+/-0.043) for {'n_neighbors': 100, 'algorithm': 'brute', 'p': 2}\n",
      "0.610 (+/-0.039) for {'n_neighbors': 100, 'algorithm': 'brute', 'p': 9}\n",
      "0.661 (+/-0.032) for {'n_neighbors': 125, 'algorithm': 'brute', 'p': 1}\n",
      "0.634 (+/-0.028) for {'n_neighbors': 125, 'algorithm': 'brute', 'p': 2}\n",
      "0.606 (+/-0.045) for {'n_neighbors': 125, 'algorithm': 'brute', 'p': 9}\n",
      "0.651 (+/-0.032) for {'n_neighbors': 200, 'algorithm': 'brute', 'p': 1}\n",
      "0.619 (+/-0.034) for {'n_neighbors': 200, 'algorithm': 'brute', 'p': 2}\n",
      "0.595 (+/-0.050) for {'n_neighbors': 200, 'algorithm': 'brute', 'p': 9}\n",
      "0.642 (+/-0.035) for {'n_neighbors': 250, 'algorithm': 'brute', 'p': 1}\n",
      "0.615 (+/-0.024) for {'n_neighbors': 250, 'algorithm': 'brute', 'p': 2}\n",
      "0.586 (+/-0.053) for {'n_neighbors': 250, 'algorithm': 'brute', 'p': 9}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         24       0.72      0.73      0.72       342\n",
      "        368       0.68      0.75      0.71       334\n",
      "       1793       0.73      0.64      0.68       335\n",
      "\n",
      "avg / total       0.71      0.71      0.71      1011\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = test_labels, clf.best_estimator_.predict(test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "# n_neighbors': 30, 'algorithm': 'auto', 'p': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = test_labels, loaded_knn_pca.best_estimator_.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24, 1793, 1793, ..., 1793,   24, 1793])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>knn_pred</th>\n",
       "      <th>knn_res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>24</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>24</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9724</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9726</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9730</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9732</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9746</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9754</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual  knn_pred knn_res\n",
       "33        24        24    True\n",
       "34      1793      1793    True\n",
       "35      1793      1793    True\n",
       "37      1793        24   False\n",
       "101     1793       368   False\n",
       "102     1793      1793    True\n",
       "105     1793      1793    True\n",
       "217     1793        24   False\n",
       "261     1793      1793    True\n",
       "295     1793       368   False\n",
       "354     1793       368   False\n",
       "355     1793        24   False\n",
       "399     1793      1793    True\n",
       "474     1793      1793    True\n",
       "507     1793       368   False\n",
       "545     1793      1793    True\n",
       "554       24        24    True\n",
       "604       24        24    True\n",
       "607       24       368   False\n",
       "608       24        24    True\n",
       "662       24        24    True\n",
       "686     1793      1793    True\n",
       "710     1793      1793    True\n",
       "860       24       368   False\n",
       "861       24        24    True\n",
       "862       24      1793   False\n",
       "863       24        24    True\n",
       "864       24        24    True\n",
       "865       24        24    True\n",
       "866       24        24    True\n",
       "...      ...       ...     ...\n",
       "9704    1793      1793    True\n",
       "9715    1793       368   False\n",
       "9716    1793       368   False\n",
       "9718    1793        24   False\n",
       "9719    1793       368   False\n",
       "9720    1793      1793    True\n",
       "9724    1793      1793    True\n",
       "9726    1793      1793    True\n",
       "9727    1793      1793    True\n",
       "9729    1793        24   False\n",
       "9730    1793      1793    True\n",
       "9731    1793        24   False\n",
       "9732      24        24    True\n",
       "9734      24        24    True\n",
       "9735    1793      1793    True\n",
       "9739      24        24    True\n",
       "9740      24        24    True\n",
       "9744      24      1793   False\n",
       "9746      24        24    True\n",
       "9749      24        24    True\n",
       "9750      24        24    True\n",
       "9751      24        24    True\n",
       "9752      24        24    True\n",
       "9753      24        24    True\n",
       "9754      24        24    True\n",
       "9755      24        24    True\n",
       "9756      24        24    True\n",
       "9757      24      1793   False\n",
       "9758      24        24    True\n",
       "9759      24      1793   False\n",
       "\n",
       "[1011 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_test_df = fns.result_table(y_true, y_pred)\n",
    "knn_test_df = knn_test_df.rename(index=str, columns={'predictions': 'knn_pred', 'results': 'knn_res'})\n",
    "knn_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">knn_res</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_pred</th>\n",
       "      <th>24</th>\n",
       "      <th>368</th>\n",
       "      <th>1793</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>248</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>49</td>\n",
       "      <td>252</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         knn_res          \n",
       "knn_pred    24   368  1793\n",
       "actual                    \n",
       "24           248   49   45\n",
       "368           49  252   33\n",
       "1793          49   71  215"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knn_test_df.groupby(['actual', 'knn_pred']).aggregate({'knn_res': 'count'}).unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707220573689\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# # save the model to disk\n",
    "# filename = 'models/knn.sav'\n",
    "# pickle.dump(clf, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_knn_pca = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_knn_pca.score(test, test_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29277942631058357"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results.insert(0,'actual',test_labels)\n",
    "results.insert(1,'predictions',y_pred)\n",
    "\n",
    "misclassified = (results['actual'] != results['predictions']).mean()\n",
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Give me the 5 nearest neighbors of the first two items in the validation set\n",
    "pd.concat((test_labels.iloc[:5].reset_index(drop = True), \n",
    "           pd.DataFrame(clf.kneighbors(test[0:5], n_neighbors=3, return_distance=False)).\n",
    "        applymap(lambda x: fn_train.iloc[x])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get neighbors \n",
    "prediction_nearest_neighbors = pd.DataFrame(clf.kneighbors(test, n_neighbors=107, return_distance=False))\n",
    "prediction_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbors = list(model.kneighbors(test_data_img[5], n_neighbors=31, return_distance=False)[0])\n",
    "NN_images = [y_train.iloc[x] for x in nearest_neighbors]\n",
    "[ (i,NN_images.count(i)) for i in set(NN_images) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "======================================================================================================\n",
    "\n",
    "# PCA\n",
    "\n",
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get 10 principal components\n",
    "pca = PCA(n_components=15)\n",
    "pca.fit(train)\n",
    "pca_transformed = pca.transform(train)\n",
    "pca_transformed_test = pca.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98238212024365701"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEZCAYAAACJjGL9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f062e00bf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title(\"Scree Plot: 10 Principal Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"% of Explained Variance\")\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "======================================================================================================\n",
    "\n",
    "# KNN + PCA\n",
    "\n",
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'n_neighbors': [2, 3, 4, 5, 10, 15, 20], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'p': [1, 2, 9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the KNN model\n",
    "print \"Training KNN Model...\"\n",
    "\n",
    "# minkowski_distance\n",
    "k = [2, 3, 4, 5, 10, 15, 20]\n",
    "#k = [5, 20]\n",
    "parameters = {'n_neighbors': k, 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1, 2, 9]\n",
    "              }\n",
    "knn_pca = KNeighborsClassifier()\n",
    "clf_pca = GridSearchCV(knn_pca, parameters, cv=6, n_jobs=4)\n",
    "clf_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Costed time: \n",
      "443.537456\n",
      "Accuracy of the Model on training set:0.703859475507\n",
      "Accuracy of the Model on testing set:0.686449060336\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_pca.fit(pca_transformed, train_labels)\n",
    "\n",
    "print('-' * 53)\n",
    "print 'Costed time: \\n%f' % (time.time() - start_time)\n",
    "\n",
    "\n",
    "# accuracy for the train set\n",
    "print \"Accuracy of the Model on training set:\" + str(clf_pca.score(pca_transformed,train_labels))\n",
    "\n",
    "# accuracy for the test set\n",
    "print \"Accuracy of the Model on testing set:\" + str(clf_pca.score(pca_transformed_test,test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "()\n",
      "{'n_neighbors': 20, 'algorithm': 'auto', 'p': 2}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.568 (+/-0.028) for {'n_neighbors': 2, 'algorithm': 'auto', 'p': 1}\n",
      "0.580 (+/-0.024) for {'n_neighbors': 2, 'algorithm': 'auto', 'p': 2}\n",
      "0.567 (+/-0.025) for {'n_neighbors': 2, 'algorithm': 'auto', 'p': 9}\n",
      "0.607 (+/-0.025) for {'n_neighbors': 3, 'algorithm': 'auto', 'p': 1}\n",
      "0.610 (+/-0.014) for {'n_neighbors': 3, 'algorithm': 'auto', 'p': 2}\n",
      "0.606 (+/-0.038) for {'n_neighbors': 3, 'algorithm': 'auto', 'p': 9}\n",
      "0.615 (+/-0.045) for {'n_neighbors': 4, 'algorithm': 'auto', 'p': 1}\n",
      "0.624 (+/-0.029) for {'n_neighbors': 4, 'algorithm': 'auto', 'p': 2}\n",
      "0.608 (+/-0.047) for {'n_neighbors': 4, 'algorithm': 'auto', 'p': 9}\n",
      "0.622 (+/-0.041) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 1}\n",
      "0.634 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 2}\n",
      "0.621 (+/-0.034) for {'n_neighbors': 5, 'algorithm': 'auto', 'p': 9}\n",
      "0.648 (+/-0.048) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 1}\n",
      "0.644 (+/-0.057) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 2}\n",
      "0.628 (+/-0.054) for {'n_neighbors': 10, 'algorithm': 'auto', 'p': 9}\n",
      "0.647 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 1}\n",
      "0.653 (+/-0.060) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 2}\n",
      "0.642 (+/-0.048) for {'n_neighbors': 15, 'algorithm': 'auto', 'p': 9}\n",
      "0.651 (+/-0.054) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 1}\n",
      "0.654 (+/-0.051) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 2}\n",
      "0.642 (+/-0.045) for {'n_neighbors': 20, 'algorithm': 'auto', 'p': 9}\n",
      "0.568 (+/-0.028) for {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.580 (+/-0.024) for {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.567 (+/-0.025) for {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.607 (+/-0.025) for {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.610 (+/-0.014) for {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.606 (+/-0.038) for {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.615 (+/-0.045) for {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.624 (+/-0.029) for {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.608 (+/-0.047) for {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.622 (+/-0.041) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.634 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.621 (+/-0.034) for {'n_neighbors': 5, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.648 (+/-0.048) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.644 (+/-0.057) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.628 (+/-0.054) for {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.647 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.653 (+/-0.060) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.642 (+/-0.048) for {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.651 (+/-0.054) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 1}\n",
      "0.654 (+/-0.051) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 2}\n",
      "0.642 (+/-0.045) for {'n_neighbors': 20, 'algorithm': 'ball_tree', 'p': 9}\n",
      "0.568 (+/-0.028) for {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.580 (+/-0.024) for {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.567 (+/-0.025) for {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.607 (+/-0.025) for {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.610 (+/-0.014) for {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.606 (+/-0.038) for {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.615 (+/-0.045) for {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.624 (+/-0.029) for {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.608 (+/-0.047) for {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.622 (+/-0.041) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.634 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.621 (+/-0.034) for {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.648 (+/-0.048) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.644 (+/-0.057) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.628 (+/-0.054) for {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.647 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.653 (+/-0.060) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.642 (+/-0.048) for {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.651 (+/-0.054) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 1}\n",
      "0.654 (+/-0.051) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 2}\n",
      "0.642 (+/-0.045) for {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 9}\n",
      "0.568 (+/-0.028) for {'n_neighbors': 2, 'algorithm': 'brute', 'p': 1}\n",
      "0.580 (+/-0.024) for {'n_neighbors': 2, 'algorithm': 'brute', 'p': 2}\n",
      "0.567 (+/-0.025) for {'n_neighbors': 2, 'algorithm': 'brute', 'p': 9}\n",
      "0.607 (+/-0.025) for {'n_neighbors': 3, 'algorithm': 'brute', 'p': 1}\n",
      "0.610 (+/-0.014) for {'n_neighbors': 3, 'algorithm': 'brute', 'p': 2}\n",
      "0.606 (+/-0.038) for {'n_neighbors': 3, 'algorithm': 'brute', 'p': 9}\n",
      "0.615 (+/-0.045) for {'n_neighbors': 4, 'algorithm': 'brute', 'p': 1}\n",
      "0.624 (+/-0.029) for {'n_neighbors': 4, 'algorithm': 'brute', 'p': 2}\n",
      "0.608 (+/-0.047) for {'n_neighbors': 4, 'algorithm': 'brute', 'p': 9}\n",
      "0.622 (+/-0.041) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 1}\n",
      "0.634 (+/-0.049) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 2}\n",
      "0.621 (+/-0.034) for {'n_neighbors': 5, 'algorithm': 'brute', 'p': 9}\n",
      "0.648 (+/-0.048) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 1}\n",
      "0.644 (+/-0.057) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 2}\n",
      "0.628 (+/-0.054) for {'n_neighbors': 10, 'algorithm': 'brute', 'p': 9}\n",
      "0.647 (+/-0.058) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 1}\n",
      "0.653 (+/-0.060) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 2}\n",
      "0.642 (+/-0.048) for {'n_neighbors': 15, 'algorithm': 'brute', 'p': 9}\n",
      "0.651 (+/-0.054) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 1}\n",
      "0.654 (+/-0.051) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 2}\n",
      "0.642 (+/-0.045) for {'n_neighbors': 20, 'algorithm': 'brute', 'p': 9}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         24       0.73      0.71      0.72       342\n",
      "        368       0.66      0.69      0.67       334\n",
      "       1793       0.67      0.66      0.67       335\n",
      "\n",
      "avg / total       0.69      0.69      0.69      1011\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf_pca.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf_pca.cv_results_['mean_test_score']\n",
    "stds = clf_pca.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf_pca.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "knn_pca_true, knn_pca_pred = test_labels, clf_pca.best_estimator_.predict(pca_transformed_test)\n",
    "print(classification_report(knn_pca_true, knn_pca_pred))\n",
    "print()\n",
    "\n",
    "# n_neighbors': 20, 'algorithm': 'auto', 'p': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>knn_pca_pred</th>\n",
       "      <th>knn_pca_res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>24</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>24</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9724</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9726</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9730</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9732</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9746</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9754</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual  knn_pca_pred knn_pca_res\n",
       "33        24            24        True\n",
       "34      1793          1793        True\n",
       "35      1793          1793        True\n",
       "37      1793            24       False\n",
       "101     1793          1793        True\n",
       "102     1793          1793        True\n",
       "105     1793          1793        True\n",
       "217     1793            24       False\n",
       "261     1793          1793        True\n",
       "295     1793           368       False\n",
       "354     1793            24       False\n",
       "355     1793            24       False\n",
       "399     1793          1793        True\n",
       "474     1793          1793        True\n",
       "507     1793           368       False\n",
       "545     1793          1793        True\n",
       "554       24            24        True\n",
       "604       24            24        True\n",
       "607       24           368       False\n",
       "608       24            24        True\n",
       "662       24            24        True\n",
       "686     1793          1793        True\n",
       "710     1793          1793        True\n",
       "860       24           368       False\n",
       "861       24            24        True\n",
       "862       24          1793       False\n",
       "863       24            24        True\n",
       "864       24            24        True\n",
       "865       24            24        True\n",
       "866       24            24        True\n",
       "...      ...           ...         ...\n",
       "9704    1793          1793        True\n",
       "9715    1793           368       False\n",
       "9716    1793           368       False\n",
       "9718    1793            24       False\n",
       "9719    1793           368       False\n",
       "9720    1793          1793        True\n",
       "9724    1793          1793        True\n",
       "9726    1793          1793        True\n",
       "9727    1793          1793        True\n",
       "9729    1793           368       False\n",
       "9730    1793          1793        True\n",
       "9731    1793            24       False\n",
       "9732      24            24        True\n",
       "9734      24            24        True\n",
       "9735    1793          1793        True\n",
       "9739      24            24        True\n",
       "9740      24            24        True\n",
       "9744      24          1793       False\n",
       "9746      24            24        True\n",
       "9749      24            24        True\n",
       "9750      24            24        True\n",
       "9751      24          1793       False\n",
       "9752      24            24        True\n",
       "9753      24            24        True\n",
       "9754      24            24        True\n",
       "9755      24            24        True\n",
       "9756      24            24        True\n",
       "9757      24          1793       False\n",
       "9758      24            24        True\n",
       "9759      24          1793       False\n",
       "\n",
       "[1011 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_knn_test_df = fns.result_table(knn_pca_true, knn_pca_pred)\n",
    "pca_knn_test_df = pca_knn_test_df.rename(index=str, columns={'predictions': 'knn_pca_pred', 'results': 'knn_pca_res'})\n",
    "pca_knn_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">knn_pca_res</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_pca_pred</th>\n",
       "      <th>24</th>\n",
       "      <th>368</th>\n",
       "      <th>1793</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>243</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>54</td>\n",
       "      <td>229</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>37</td>\n",
       "      <td>76</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             knn_pca_res          \n",
       "knn_pca_pred        24   368  1793\n",
       "actual                            \n",
       "24                   243   43   56\n",
       "368                   54  229   51\n",
       "1793                  37   76  222"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_knn_test_df.groupby(['actual', 'knn_pca_pred']).aggregate({'knn_pca_res': 'count'}).unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>knn_pred</th>\n",
       "      <th>knn_res</th>\n",
       "      <th>knn_pca_pred</th>\n",
       "      <th>knn_pca_res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340715</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340716</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340717</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340718</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340719</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340720</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340721</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340722</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340723</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340724</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340725</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340726</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340727</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340728</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340729</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340730</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340731</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340732</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340733</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340734</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340735</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340736</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>1793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340737</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340738</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340739</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340740</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340741</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340742</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340743</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340744</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340745 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual  knn_pred knn_res  knn_pca_pred knn_pca_res\n",
       "0           24        24    True            24        True\n",
       "1           24        24    True            24        True\n",
       "2           24        24    True            24        True\n",
       "3           24        24    True           368       False\n",
       "4           24        24    True            24        True\n",
       "5           24        24    True            24        True\n",
       "6           24        24    True           368       False\n",
       "7           24        24    True            24        True\n",
       "8           24        24    True          1793       False\n",
       "9           24        24    True            24        True\n",
       "10          24        24    True            24        True\n",
       "11          24        24    True            24        True\n",
       "12          24        24    True            24        True\n",
       "13          24        24    True          1793       False\n",
       "14          24        24    True            24        True\n",
       "15          24        24    True            24        True\n",
       "16          24        24    True            24        True\n",
       "17          24        24    True            24        True\n",
       "18          24        24    True            24        True\n",
       "19          24        24    True            24        True\n",
       "20          24        24    True            24        True\n",
       "21          24        24    True            24        True\n",
       "22          24        24    True            24        True\n",
       "23          24        24    True            24        True\n",
       "24          24        24    True           368       False\n",
       "25          24        24    True            24        True\n",
       "26          24        24    True            24        True\n",
       "27          24        24    True            24        True\n",
       "28          24        24    True           368       False\n",
       "29          24        24    True            24        True\n",
       "...        ...       ...     ...           ...         ...\n",
       "340715     368       368    True           368        True\n",
       "340716     368       368    True           368        True\n",
       "340717     368       368    True           368        True\n",
       "340718     368       368    True           368        True\n",
       "340719     368       368    True           368        True\n",
       "340720     368       368    True           368        True\n",
       "340721     368       368    True          1793       False\n",
       "340722     368       368    True           368        True\n",
       "340723     368       368    True           368        True\n",
       "340724     368       368    True           368        True\n",
       "340725     368       368    True          1793       False\n",
       "340726     368       368    True           368        True\n",
       "340727     368       368    True           368        True\n",
       "340728     368       368    True           368        True\n",
       "340729     368       368    True           368        True\n",
       "340730     368       368    True           368        True\n",
       "340731     368       368    True           368        True\n",
       "340732     368       368    True          1793       False\n",
       "340733     368       368    True           368        True\n",
       "340734     368       368    True            24       False\n",
       "340735     368       368    True          1793       False\n",
       "340736     368       368    True          1793       False\n",
       "340737     368       368    True           368        True\n",
       "340738     368       368    True           368        True\n",
       "340739     368       368    True           368        True\n",
       "340740     368       368    True            24       False\n",
       "340741     368       368    True           368        True\n",
       "340742     368       368    True           368        True\n",
       "340743     368       368    True            24       False\n",
       "340744     368       368    True           368        True\n",
       "\n",
       "[340745 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_com = knn_test_df.merge(pca_knn_test_df, how='inner', on='actual')\n",
    "knn_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "knn_com.to_csv('data/knn_com.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686449060336\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'models/knn_pca.sav'\n",
    "pickle.dump(clf_pca, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_knn_pca = pickle.load(open(filename, 'rb'))\n",
    "result_pca = loaded_knn_pca.score(pca_transformed_test, test_labels)\n",
    "print(result_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The base estimator doesn't support sample weight",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a8c75585db12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 373\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                               \"sample_weight\")\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport_sample_weight\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The base estimator doesn't support sample weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Build estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The base estimator doesn't support sample weight"
     ]
    }
   ],
   "source": [
    "class_weights #dict\n",
    "sample_weights = map(lambda x: class_weights[x], train_labels)\n",
    "\n",
    "cls = BaggingClassifier(KNeighborsClassifier(n_neighbors=30, p = 1), n_estimators=20)\n",
    "cls.fit(train, train_labels, sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 1.0905070000365538,\n",
       " 1.2399418121363259,\n",
       " 1.2399418121363259,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " 0.78338847749592988,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = map(lambda x: class_weights[x], train_labels)\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_top_movement(num_movements = 3):\n",
    "    movement_hist_train = pd.read_csv('data/movement_hist_train.csv')\n",
    "    movement_hist_test = pd.read_csv('data/movement_hist_test.csv')\n",
    "    \n",
    "    print \"[INFO] The size of train histogram for Random Forest\" + str(movement_hist_train.shape)\n",
    "    print \"[INFO] The size of test histogram for Random Forest\" + str(movement_hist_test.shape)\n",
    "\n",
    "    movement_hist_train.iloc[:,3:-1] = movement_hist_train.iloc[:, 3:-1]\\\n",
    "        .apply(lambda x: x.astype(np.float) / (x.sum()/3), axis = 1, raw = True)\n",
    "\n",
    "    movement_hist_test.iloc[:,3:-1] = movement_hist_test.iloc[:, 3:-1]\\\n",
    "            .apply(lambda x: x.astype(np.float) / (x.sum()/3), axis = 1, raw = True)\n",
    "    \n",
    "    mv_index = movement_hist_train['sup_art_movement'].value_counts().index[:num_movements]\n",
    "    train = movement_hist_train[movement_hist_train['sup_art_movement'].isin(mv_index)]\n",
    "    test = movement_hist_test[movement_hist_test['sup_art_movement'].isin(mv_index)]\n",
    "    \n",
    "    train_label = train['sup_art_movement']\n",
    "    test_label = test['sup_art_movement']\n",
    "    \n",
    "    print 'top movement for train:\\n %s ' % str(train['sup_art_movement'].value_counts())\n",
    "    print '-' * 50\n",
    "    print 'top movement for test:\\n %s ' % str(test['sup_art_movement'].value_counts())\n",
    "\n",
    "    test = test.drop(['author_id', 'painting_id', 'sup_art_movement'], axis=1)\n",
    "    train = train.drop(['author_id', 'painting_id', 'sup_art_movement'], axis=1)\n",
    "    \n",
    "    return train, train_label, test, test_label\n",
    "\n",
    "\n",
    "def movement_encod(train_labels, test_labels):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_labels)\n",
    "    le.classes_\n",
    "    train_labels_encd = le.transform(train_labels)\n",
    "    test_labels_encd = le.transform(test_labels)\n",
    "    \n",
    "    print \"[INFO] the original train labels: %s\" % str(train_labels.unique())\n",
    "    print \"[INFO] the encoded labels: %s\" % str(train_labels_encd)\n",
    "    \n",
    "    print '-' * 50\n",
    "    print \"[INFO] the original train labels: %s\" % str(test_labels.unique())\n",
    "    print \"[INFO] the encoded labels: %s\" % str(test_labels_encd)\n",
    "    \n",
    "    return train_labels_encd, test_labels_encd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The size of train histogram for Random Forest(67059, 34)\n",
      "[INFO] The size of test histogram for Random Forest(28740, 34)\n",
      "top movement for train:\n",
      " Impressionist    12694\n",
      "Baroque           9119\n",
      "Realist           8020\n",
      "Name: sup_art_movement, dtype: int64 \n",
      "--------------------------------------------------\n",
      "top movement for test:\n",
      " Impressionist    5441\n",
      "Baroque          3909\n",
      "Realist          3438\n",
      "Name: sup_art_movement, dtype: int64 \n",
      "[INFO] the original train labels: ['Impressionist' 'Realist' 'Baroque']\n",
      "[INFO] the encoded labels: [1 2 1 ..., 1 1 1]\n",
      "--------------------------------------------------\n",
      "[INFO] the original train labels: ['Impressionist' 'Baroque' 'Realist']\n",
      "[INFO] the encoded labels: [1 0 2 ..., 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "train, train_labels_y, test, test_labels_y = get_top_movement(3)\n",
    "train_labels, test_labels = movement_encod(train_labels_y, test_labels_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0905070000365538, 1: 0.78338847749592988, 2: 1.2399418121363259}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movement_hist_train = pd.read_csv('data/movement_hist_train.csv')\n",
    "counts = np.bincount(train_labels)\n",
    "avg_count = counts.mean()\n",
    "class_weights = avg_count / counts\n",
    "class_weights = dict(zip(range(len(class_weights)), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
