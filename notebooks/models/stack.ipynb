{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athenaeum_authors.csv\n",
      "athenaeum_authors_preview.csv\n",
      "athenaeum_painting_filtered.csv\n",
      "athenaeum_painting_movement.csv\n",
      "athenaeum_painting_movement_test.csv\n",
      "athenaeum_painting_movement_train.csv\n",
      "athenaeum_paintings.csv\n",
      "athenaeum_paintings_sizes.csv\n",
      "color_hist_kmeans_206552.csv\n",
      "color_histograms.csv\n",
      "color_hist_size_206552.csv\n",
      "complete_data.csv\n",
      "extra_tree_com.csv\n",
      "grad_boost_com.csv\n",
      "images\n",
      "images_athenaeum\n",
      "images_sizes_2325.csv\n",
      "kmeans_centers.csv\n",
      "kmeans.png\n",
      "kmeans_tsne.png\n",
      "knn_com.csv\n",
      "model_accuracy.csv\n",
      "movement_hist_test.csv\n",
      "movement_hist_train.csv\n",
      "nbc_com.csv\n",
      "net1_ensemble_stacking.csv\n",
      "net_predicted.csv\n",
      "nn_pca_test.csv\n",
      "nn_pca_train.csv\n",
      "painter_info_clean.csv\n",
      "painting_info_clean.csv\n",
      "pca20_kmeans_test.csv\n",
      "pca20_kmeans_train.csv\n",
      "resized_200\n",
      "rf_com.csv\n",
      "test_author200.csv\n",
      "test_data.csv\n",
      "test_hist_author_knn.csv\n",
      "test_hist_author_rf.csv\n",
      "train_author200.csv\n",
      "train_data.csv\n",
      "train_hist_author_knn.csv\n",
      "train_hist_author_rf.csv\n",
      "xgb_com.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src import fns_models as fns\n",
    "\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The size of train histogram for Random Forest(49890, 35)\n",
      "[INFO] The size of test histogram for Random Forest(12473, 35)\n",
      "24      1369\n",
      "1793    1338\n",
      "368     1335\n",
      "Name: author_id, dtype: int64\n",
      "[trian above] ==================================================[test below]\n",
      "24      342\n",
      "1793    335\n",
      "368     334\n",
      "Name: author_id, dtype: int64\n",
      "(4042,)\n",
      "(4042, 35)\n"
     ]
    }
   ],
   "source": [
    "train, train_labels, test, test_labels = fns.get_top_author(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = train.sample(500)\n",
    "train_labels = train_labels.sample(500)\n",
    "test = test.sample(500)\n",
    "test_labels = test_labels.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "============================================================================================\n",
    "\n",
    "# stacking \n",
    "\n",
    "[stacking](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)\n",
    "\n",
    "============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train\n",
    "y = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 31)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.67 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.66 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.57 (+/- 0.03) [Naive Bayes]\n",
      "Accuracy: 0.68 (+/- 0.01) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "# stacking he class-probabilities of the first-level classifiers can be used to train the meta-classifier (2nd-level classifier)\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=30, algorithm='auto', p=1 )\n",
    "clf2 = RandomForestClassifier(random_state=2017,max_features=7, n_estimators=654, max_depth=6)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500 +/- 0.15 {'meta-logisticregression__C': 0.1, 'randomforestclassifier__n_estimators': 10, 'kneighborsclassifier__n_neighbors': 1}\n",
      "0.350 +/- 0.11 {'meta-logisticregression__C': 0.1, 'randomforestclassifier__n_estimators': 50, 'kneighborsclassifier__n_neighbors': 1}\n",
      "0.500 +/- 0.15 {'meta-logisticregression__C': 10.0, 'randomforestclassifier__n_estimators': 10, 'kneighborsclassifier__n_neighbors': 1}\n",
      "0.400 +/- 0.13 {'meta-logisticregression__C': 10.0, 'randomforestclassifier__n_estimators': 50, 'kneighborsclassifier__n_neighbors': 1}\n",
      "0.450 +/- 0.08 {'meta-logisticregression__C': 0.1, 'randomforestclassifier__n_estimators': 10, 'kneighborsclassifier__n_neighbors': 5}\n",
      "0.300 +/- 0.07 {'meta-logisticregression__C': 0.1, 'randomforestclassifier__n_estimators': 50, 'kneighborsclassifier__n_neighbors': 5}\n",
      "0.400 +/- 0.07 {'meta-logisticregression__C': 10.0, 'randomforestclassifier__n_estimators': 10, 'kneighborsclassifier__n_neighbors': 5}\n",
      "0.300 +/- 0.07 {'meta-logisticregression__C': 10.0, 'randomforestclassifier__n_estimators': 50, 'kneighborsclassifier__n_neighbors': 5}\n",
      "Best parameters: {'meta-logisticregression__C': 0.1, 'randomforestclassifier__n_estimators': 10, 'kneighborsclassifier__n_neighbors': 1}\n",
      "Accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "# stacking using GridSearch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "# Initializing models\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "          'randomforestclassifier__n_estimators': [10, 50],\n",
    "          'meta-logisticregression__C': [0.1, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=sclf, \n",
    "                    param_grid=params, \n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "grid.fit(X, y)\n",
    "\n",
    "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    print(\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "print('Best parameters: %s' % grid.best_params_)\n",
    "print('Accuracy: %.2f' % grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stacking using baysian optimization\n",
    "# stacking using GridSearch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "def stack_fn(sclf, params):\n",
    "    \n",
    "\n",
    "# Initializing models\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "          'randomforestclassifier__n_estimators': [10, 50],\n",
    "          'meta-logisticregression__C': [0.1, 10.0]}\n",
    "\n",
    "# grid = GridSearchCV(estimator=sclf, \n",
    "#                     param_grid=params, \n",
    "#                     cv=5,\n",
    "#                     refit=True)\n",
    "\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "random_state = 2017\n",
    "    \n",
    "sBO = BayesianOptimization(sclf, params)\n",
    "sBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "\n",
    "print \"Bayesian Optimization Best Score: %d\" % sBO.res['max']['max_val']\n",
    "\n",
    "print \"Bayesian Optimization Best Parameters: %s\" % str(sBO.res['max']['max_params'])\n",
    "\n",
    "print (lrBO.res['max'])\n",
    "\n",
    "fns.plot_bo(lr_fnc, lrBO)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stacking LG + RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "============================================================================================\n",
    "\n",
    "# stacking xgboost\n",
    "\n",
    "[xgboost stacking](https://github.com/AntonUBC/kaggle_flavours_of_physics)\n",
    "\n",
    "============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4042, 74)\n",
      "(1011, 74)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>painting_id</th>\n",
       "      <th>nn_01</th>\n",
       "      <th>nn_02</th>\n",
       "      <th>nn_03</th>\n",
       "      <th>nn_04</th>\n",
       "      <th>nn_05</th>\n",
       "      <th>nn_06</th>\n",
       "      <th>nn_07</th>\n",
       "      <th>nn_08</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_11</th>\n",
       "      <th>pca_12</th>\n",
       "      <th>pca_13</th>\n",
       "      <th>pca_14</th>\n",
       "      <th>pca_15</th>\n",
       "      <th>pca_16</th>\n",
       "      <th>pca_17</th>\n",
       "      <th>pca_18</th>\n",
       "      <th>pca_19</th>\n",
       "      <th>kmeans_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1793</td>\n",
       "      <td>89493</td>\n",
       "      <td>1.209929</td>\n",
       "      <td>0.320030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686977</td>\n",
       "      <td>0.80809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044231</td>\n",
       "      <td>-0.021356</td>\n",
       "      <td>-0.013008</td>\n",
       "      <td>-0.002617</td>\n",
       "      <td>0.025611</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>-0.023150</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368</td>\n",
       "      <td>53260</td>\n",
       "      <td>0.473508</td>\n",
       "      <td>0.140545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960494</td>\n",
       "      <td>0.76018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076586</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.194509</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>0.041777</td>\n",
       "      <td>0.079497</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>-0.035633</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id  painting_id     nn_01     nn_02  nn_03  nn_04     nn_05  \\\n",
       "0       1793        89493  1.209929  0.320030    0.0    0.0  0.686977   \n",
       "1        368        53260  0.473508  0.140545    0.0    0.0  0.960494   \n",
       "\n",
       "     nn_06  nn_07     nn_08      ...          pca_11    pca_12    pca_13  \\\n",
       "0  0.80809    0.0  0.298580      ...       -0.044231 -0.021356 -0.013008   \n",
       "1  0.76018    0.0  0.514217      ...       -0.076586  0.011988  0.194509   \n",
       "\n",
       "     pca_14    pca_15    pca_16    pca_17    pca_18    pca_19  kmeans_labels  \n",
       "0 -0.002617  0.025611  0.015427 -0.023150  0.001554 -0.004448              2  \n",
       "1 -0.016484  0.041777  0.079497  0.031914 -0.035633 -0.009105              3  \n",
       "\n",
       "[2 rows x 74 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pca_train = pd.read_csv('data/nn_pca_train.csv')\n",
    "nn_pca_test = pd.read_csv('data/nn_pca_test.csv')\n",
    "\n",
    "print nn_pca_train.shape\n",
    "print nn_pca_test.shape\n",
    "\n",
    "nn_pca_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'author_id', u'painting_id', u'nn_01', u'nn_02', u'nn_03', u'nn_04',\n",
       "       u'nn_05', u'nn_06', u'nn_07', u'nn_08', u'nn_09', u'nn_10', u'nn_11',\n",
       "       u'nn_12', u'nn_13', u'nn_14', u'nn_15', u'nn_16', u'nn_17', u'nn_18',\n",
       "       u'nn_19', u'nn_20', u'nn_21', u'nn_22', u'nn_23', u'nn_24', u'nn_25',\n",
       "       u'nn_26', u'nn_27', u'nn_28', u'nn_29', u'nn_30', u'nn_31', u'nn_32',\n",
       "       u'nn_33', u'nn_34', u'nn_35', u'nn_36', u'nn_37', u'nn_38', u'nn_39',\n",
       "       u'nn_40', u'nn_41', u'nn_42', u'nn_43', u'nn_44', u'nn_45', u'nn_46',\n",
       "       u'nn_47', u'nn_48', u'nn_49', u'nn_50', u'height_width_ratio',\n",
       "       u'pca_00', u'pca_01', u'pca_02', u'pca_03', u'pca_04', u'pca_05',\n",
       "       u'pca_06', u'pca_07', u'pca_08', u'pca_09', u'pca_10', u'pca_11',\n",
       "       u'pca_12', u'pca_13', u'pca_14', u'pca_15', u'pca_16', u'pca_17',\n",
       "       u'pca_18', u'pca_19', u'kmeans_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pca_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels = nn_pca_train['author_id']\n",
    "test_labels = nn_pca_test['author_id']\n",
    "\n",
    "train = nn_pca_train.iloc[:, 2:]\n",
    "test = nn_pca_test.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4042, 84)\n",
      "(1011, 84)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>painting_id</th>\n",
       "      <th>nn_01</th>\n",
       "      <th>nn_02</th>\n",
       "      <th>nn_03</th>\n",
       "      <th>nn_04</th>\n",
       "      <th>nn_05</th>\n",
       "      <th>nn_06</th>\n",
       "      <th>nn_07</th>\n",
       "      <th>nn_08</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_23</th>\n",
       "      <th>hist_24</th>\n",
       "      <th>hist_25</th>\n",
       "      <th>hist_26</th>\n",
       "      <th>hist_27</th>\n",
       "      <th>hist_28</th>\n",
       "      <th>hist_29</th>\n",
       "      <th>hist_30</th>\n",
       "      <th>height_width_ratio</th>\n",
       "      <th>kmeans_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1793</td>\n",
       "      <td>89493</td>\n",
       "      <td>1.209929</td>\n",
       "      <td>0.32003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686977</td>\n",
       "      <td>0.80809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29692</td>\n",
       "      <td>0.054243</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.1736</td>\n",
       "      <td>0.197649</td>\n",
       "      <td>0.110999</td>\n",
       "      <td>0.208883</td>\n",
       "      <td>0.308868</td>\n",
       "      <td>1.521395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id  painting_id     nn_01    nn_02  nn_03  nn_04     nn_05    nn_06  \\\n",
       "0       1793        89493  1.209929  0.32003    0.0    0.0  0.686977  0.80809   \n",
       "\n",
       "   nn_07    nn_08      ...        hist_23   hist_24   hist_25  hist_26  \\\n",
       "0    0.0  0.29858      ...        0.29692  0.054243  0.000393   0.1736   \n",
       "\n",
       "    hist_27   hist_28   hist_29   hist_30  height_width_ratio  kmeans_labels  \n",
       "0  0.197649  0.110999  0.208883  0.308868            1.521395              1  \n",
       "\n",
       "[1 rows x 84 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_kmeans_train = pd.read_csv('data/nn_kmeans_train.csv')\n",
    "nn_kmeans_test = pd.read_csv('data/nn_kmeans_test.csv')\n",
    "\n",
    "print nn_kmeans_train.shape\n",
    "print nn_kmeans_test.shape\n",
    "nn_kmeans_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels = nn_kmeans_train['author_id']\n",
    "test_labels = nn_kmeans_test['author_id']\n",
    "\n",
    "train = nn_kmeans_train.iloc[:, 2:]\n",
    "test = nn_kmeans_test.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'nn_01', u'nn_02', u'nn_03', u'nn_04', u'nn_05', u'nn_06', u'nn_07',\n",
       "       u'nn_08', u'nn_09', u'nn_10', u'nn_11', u'nn_12', u'nn_13', u'nn_14',\n",
       "       u'nn_15', u'nn_16', u'nn_17', u'nn_18', u'nn_19', u'nn_20', u'nn_21',\n",
       "       u'nn_22', u'nn_23', u'nn_24', u'nn_25', u'nn_26', u'nn_27', u'nn_28',\n",
       "       u'nn_29', u'nn_30', u'nn_31', u'nn_32', u'nn_33', u'nn_34', u'nn_35',\n",
       "       u'nn_36', u'nn_37', u'nn_38', u'nn_39', u'nn_40', u'nn_41', u'nn_42',\n",
       "       u'nn_43', u'nn_44', u'nn_45', u'nn_46', u'nn_47', u'nn_48', u'nn_49',\n",
       "       u'nn_50', u'hist_01', u'hist_02', u'hist_03', u'hist_04', u'hist_05',\n",
       "       u'hist_06', u'hist_07', u'hist_08', u'hist_09', u'hist_10', u'hist_11',\n",
       "       u'hist_12', u'hist_13', u'hist_14', u'hist_15', u'hist_16', u'hist_17',\n",
       "       u'hist_18', u'hist_19', u'hist_20', u'hist_21', u'hist_22', u'hist_23',\n",
       "       u'hist_24', u'hist_25', u'hist_26', u'hist_27', u'hist_28', u'hist_29',\n",
       "       u'hist_30', u'height_width_ratio', u'kmeans_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "class XGBoostClassifier(BaseEstimator):\n",
    "    def __init__(self, nthread, eta,\n",
    "                 gamma, max_depth, min_child_weight, max_delta_step,\n",
    "                 subsample, colsample_bytree, silent, seed,\n",
    "                 l2_reg, l1_reg, n_estimators, num_class):\n",
    "        self.silent = silent\n",
    "        self.nthread = nthread\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.max_delta_step = max_delta_step\n",
    "        self.subsample = subsample\n",
    "        self.silent = silent\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.l1_reg = l1_reg\n",
    "        self.n_estimators=n_estimators\n",
    "        self.num_class = num_class\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        sf = xgb.DMatrix(X, y)\n",
    "        params = {\"objective\": 'multi:softmax',\n",
    "          \"eta\": self.eta,\n",
    "          \"gamma\": self.gamma,\n",
    "          \"max_depth\": self.max_depth,\n",
    "          \"min_child_weight\": self.min_child_weight,\n",
    "          \"max_delta_step\": self.max_delta_step,\n",
    "          \"subsample\": self.subsample,\n",
    "          \"silent\": self.silent,\n",
    "          \"colsample_bytree\": self.colsample_bytree,\n",
    "          \"seed\": self.seed,\n",
    "          \"lambda\": self.l2_reg,\n",
    "          \"alpha\": self.l1_reg,\n",
    "          \"num_class\": self.num_class}\n",
    "        self.model = xgb.train(params, sf, self.n_estimators)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X=xgb.DMatrix(X)\n",
    "        preds = self.model.predict_proba(X)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This script contains functions used for data loading, feature engineering, and saving predictions\n",
    "# It also contains a stacking function, used to obtain meta-features for the second stage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "y = train_labels\n",
    "\n",
    "def StackModels(train, test, y, clfs, n_folds, num_classes): # train data (pd data frame), test data (pd date frame), Target data,\n",
    "                                                # list of models to stack, number of folders\n",
    "\n",
    "# StackModels() performs Stacked Aggregation on data: it uses n different classifiers to get out-of-fold \n",
    "# predicted probabilities of signal for train data. It uses the whole training dataset to obtain predictions for test.\n",
    "# This procedure adds n meta-features to both train and test data (where n is number of models to stack).\n",
    "\n",
    "    print(\"Generating Meta-features\")\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "    training = train.as_matrix()\n",
    "    testing = test.as_matrix()\n",
    "#     y = y.as_matrix()\n",
    "    scaler = StandardScaler().fit(training)\n",
    "    train_all = scaler.transform(training)\n",
    "    test_all = scaler.transform(testing)\n",
    "    blend_train = np.zeros((training.shape[0], num_classes * len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((testing.shape[0], num_classes * len(clfs)))   # Number of testing data x Number of classifiers\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        \n",
    "        print ('Training classifier [%s]' % (j))\n",
    "        for i, (tr_index, cv_index) in enumerate(skf):\n",
    "            \n",
    "            print ('stacking Fold [%s] of train data' % (i))\n",
    "            \n",
    "            # This is the training and validation set (train on 2 folders, predict on a 3d folder)\n",
    "            X_train = training[tr_index]\n",
    "            Y_train = y[tr_index]\n",
    "            X_cv = training[cv_index]\n",
    "#             scaler=StandardScaler().fit(X_train)\n",
    "#             X_train=scaler.transform(X_train)\n",
    "#             X_cv=scaler.transform(X_cv)\n",
    "                                  \n",
    "            clf.fit(X_train, Y_train)\n",
    "            pred = clf.predict_proba(X_cv)\n",
    "            \n",
    "            blend_train[cv_index, (num_classes * j) : (num_classes * (j+1))] = pred\n",
    "        \n",
    "        print('stacking test data')        \n",
    "        clf.fit(train_all, y)\n",
    "        pred = clf.predict_proba(test_all)\n",
    "        \n",
    "        blend_test[:, (num_classes * j) : (num_classes * (j+1))] = pred\n",
    "\n",
    "    X_train_blend=np.concatenate((training, blend_train), axis=1)\n",
    "    X_test_blend=np.concatenate((testing, blend_test), axis=1)\n",
    "    return X_train_blend, X_test_blend, blend_train, blend_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_model_data(train_labels,test_labels):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_labels)\n",
    "    le.classes_\n",
    "    train_labels_encd = le.transform(train_labels)\n",
    "    test_labels_encd = le.transform(test_labels)\n",
    "    \n",
    "#     xgtrain = xgb.DMatrix(train, label=train_labels_encd)\n",
    "#     xgtest = xgb.DMatrix(test, label=test_labels_encd)\n",
    "    \n",
    "    return train_labels_encd, test_labels_encd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_labels, test_labels = prepare_model_data(train_labels, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model1\n",
      "Generating Meta-features\n",
      "Training classifier [0]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [1]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [2]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [3]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [4]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [5]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [6]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [7]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [8]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [9]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [10]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [11]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [12]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training classifier [13]\n",
      "stacking Fold [0] of train data\n",
      "stacking Fold [1] of train data\n",
      "stacking Fold [2] of train data\n",
      "stacking test data\n",
      "Training a XGBoost model\n",
      "Training a Random Forest model\n",
      "Training Model2\n",
      "Training a XGBoost model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "from hep_ml.losses import BinFlatnessLossFunction\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Model1(train, test, train_labels):\n",
    "    \n",
    "# Model 1 is an ensemble of XGBoost, Random Forest and Uniform Gradient Boosting Classifiers\n",
    "# which are trained using the stacked data    \n",
    "\n",
    "    model = 1    # set the model number for feature engineering\n",
    "    n_folds = 3 # set the number of folders for generating meta-features\n",
    "    n_stack = 14  # number of models used for stacking\n",
    "    \n",
    "    \n",
    "    # Initialize models for stacking\n",
    "        \n",
    "    clf1=KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf2=KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf3=KNeighborsClassifier(n_neighbors=20, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)  \n",
    "                          \n",
    "    clf4=KNeighborsClassifier(n_neighbors=40, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "                          \n",
    "    clf5=KNeighborsClassifier(n_neighbors=80, weights='uniform', algorithm='auto', leaf_size=30, \n",
    "                              p=2, metric='minkowski', metric_params=None) \n",
    "\n",
    "    clf6=KNeighborsClassifier(n_neighbors=160, weights='uniform', algorithm='auto', leaf_size=30,  \n",
    "                              p=2, metric='minkowski', metric_params=None)\n",
    "\n",
    "    clf7=KNeighborsClassifier(n_neighbors=320, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                              p=2, metric='minkowski', metric_params=None)                          \n",
    "                          \n",
    "    clf8=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=5.0, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight=None, random_state=101, solver='lbfgs', \n",
    "                            max_iter=200, multi_class='ovr', verbose=0) \n",
    "    \n",
    "    clf9=GaussianNB()\n",
    "    \n",
    "    clf10=SVC(C=5.0, kernel='rbf', degree=3, gamma=0.001, coef0=0.008, shrinking=True, probability=True, \n",
    "              tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=101)\n",
    "    \n",
    "    clf11=RandomForestClassifier(n_estimators=654, criterion='gini', max_depth=6, min_samples_split=2, \n",
    "                            min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=0.7, \n",
    "                            max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=4,\n",
    "                            random_state=101, verbose=0, warm_start=False, class_weight=None) \n",
    "    \n",
    "    clf12=ExtraTreesClassifier(n_estimators=265, criterion='gini', max_depth=6, min_samples_split=2,\n",
    "                     min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=0.7,\n",
    "                     max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=2, \n",
    "                     random_state=101, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "    clf13=GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=450, subsample=1, \n",
    "                                min_samples_split=21, min_samples_leaf=5, min_weight_fraction_leaf=0.0,\n",
    "                                max_depth=6, init=None, random_state=101, max_features=5, verbose=0,\n",
    "                                max_leaf_nodes=None, warm_start=False)\n",
    "\n",
    "#     clf14=XGBoostClassifier(nthread=2, eta=.2, gamma=2, max_depth=6, min_child_weight=18, max_delta_step=0,\n",
    "#                          subsample=0.75, colsample_bytree=0.6, silent =1, seed=101,\n",
    "#                          l2_reg=1, l1_reg=0, n_estimators=450, num_class = 3)\n",
    "    \n",
    "    clf14 = XGBClassifier(nthread = 4, learning_rate=.2, gamma=2, max_depth=6, min_child_weight=18,\n",
    "                          max_delta_step=0, subsample=0.75, colsample_bytree=0.6, silent=1, seed=101,\n",
    "                          reg_lambda=1, n_estimators=450, objective='multi:softmax')\n",
    "                               \n",
    "    clfs = [clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10, clf11, clf12, clf13, clf14]    \n",
    "        \n",
    "    # Construct stacked datasets\n",
    "#     train_blend, test_blend, train_probs, test_probs = StackModels(train[features], test[features], \n",
    "#                                                                          train.signal.values, clfs, n_folds)                                                                                      \n",
    "    \n",
    "    train_blend, test_blend, train_probs, test_probs = StackModels(train, test, train_labels, clfs, n_folds, 3)                                                                                      \n",
    "                                                                              \n",
    "    # Construct data for uniform boosting\n",
    "    columns = ['p%s ' % (i) for i in range(train_probs.shape[1])]\n",
    "    features = list(train.columns)\n",
    "    meta_train = pd.DataFrame(train_probs, columns = columns)\n",
    "    meta_test = pd.DataFrame(test_probs, columns = columns)\n",
    "#     train_ugb = pd.concat([train.reset_index(drop = True), meta_train], axis=1)\n",
    "#     test_ugb = pd.concat([test.reset_index(drop = True), meta_test], axis=1)\n",
    "#     features_ugb = features + columns               # features used for UGB training (original features + meta-features)\n",
    "\n",
    "    # Initialize models for ensemble\n",
    "#     loss = BinFlatnessLossFunction(['hist_01'], n_bins=20, power=1, fl_coefficient=3, uniform_label=0)\n",
    "                                   \n",
    "#     clf_ugb = UGradientBoostingClassifier(loss=loss, n_estimators=275, max_depth=11, min_samples_leaf=3, \n",
    "#                             learning_rate=0.03, train_features=features_ugb, subsample=0.85, random_state=101)  \n",
    "                            \n",
    "#     clf_xgb = XGBoostClassifier(nthread=6, eta=.0225, gamma=1.225, max_depth=11, min_child_weight=10, \n",
    "#                                 max_delta_step=0, subsample=0.8, colsample_bytree=0.3,\n",
    "#                                 silent =1, seed=101, l2_reg=1, l1_reg=0, n_estimators=1100, num_class = 3)\n",
    "                                \n",
    "    clf_xgb = XGBClassifier(nthread = 4, learning_rate=.0225, gamma=1.225, max_depth=11, min_child_weight=10,\n",
    "                          max_delta_step=0, subsample=0.8, colsample_bytree=0.3, silent=1, seed=101,\n",
    "                          reg_lambda=1, n_estimators=1100, objective='multi:softmax')\n",
    "    \n",
    "    clf_rf = RandomForestClassifier(n_estimators=375, criterion='gini', max_depth=10, min_samples_split=6, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=0.6, \n",
    "                                max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=4,\n",
    "                                random_state=101, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "#     # Train models\n",
    "#     print(\"Training a Uniform Gradient Boosting model\")     \n",
    "#     clf_ugb.fit(train_ugb[features_ugb + ['mass']], train_ugb['signal'])   \n",
    "#     preds_ugb = clf_ugb.predict_proba(test_ugb[features_ugb])[:,1]\n",
    "    \n",
    "#     print(\"Training a XGBoost model\")     \n",
    "#     clf_xgb.fit(train_blend, train['signal'])\n",
    "#     preds_xgb = clf_xgb.predict_proba(test_blend)\n",
    "        \n",
    "#     print(\"Training a Random Forest model\") \n",
    "#     clf_rf.fit(train_blend, train['signal'])\n",
    "#     preds_rf = clf_rf.predict_proba(test_blend)[:,1]\n",
    "    \n",
    "    \n",
    "    # Train models\n",
    "#     print (\"Training a Uniform Gradient Boosting model\")  \n",
    "#     print train_ugb.shape\n",
    "#     print train_labels.shape\n",
    "#     clf_ugb.fit(train_ugb[features_ugb], train_labels)   \n",
    "#     preds_ugb = clf_ugb.predict_proba(test_ugb[features_ugb])\n",
    "    \n",
    "    print (\"Training a XGBoost model\")     \n",
    "    clf_xgb.fit(train_blend, train_labels)\n",
    "    preds_xgb = clf_xgb.predict_proba(test_blend)\n",
    "        \n",
    "    print (\"Training a Random Forest model\") \n",
    "    clf_rf.fit(train_blend, train_labels)\n",
    "    preds_rf = clf_rf.predict_proba(test_blend)\n",
    "        \n",
    "    # Compute ensemble predictions\n",
    "#     preds = 0.3*(preds_xgb**(0.65))*(preds_rf**(0.35)) + 0.7*preds_ugb\n",
    "    preds = (preds_xgb**(0.65))*(preds_rf**(0.35))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def Model2(train, test, train_labels):\n",
    "    \n",
    "# Model 2 is a single XGBoost classifier \"undertrained\" to reduce correlation with tau-mass       \n",
    "\n",
    "    model = 2    # set the model number for feature engineering\n",
    "                                                         \n",
    "    \n",
    "    # Initialize a XGBoost model\n",
    "#     clf_xgb = XGBoostClassifier(nthread=6, eta=0.75, gamma=2, max_depth=7, min_child_weight=18, \n",
    "#                                 max_delta_step=0, subsample=0.75, colsample_bytree=0.6,\n",
    "#                                 silent=1, seed=2017, num_class = 3)\n",
    "\n",
    "    clf_xgb = XGBClassifier(nthread = 4, learning_rate=.75, gamma=2, max_depth=7, min_child_weight=18,\n",
    "                          max_delta_step=0, subsample=0.75, colsample_bytree=0.6, silent=1, seed=2017,\n",
    "                          reg_lambda=1, n_estimators=450, objective='multi:softmax')\n",
    "    \n",
    "    # Train a XGBoost model                                                                   \n",
    "    print(\"Training a XGBoost model\")  \n",
    "    clf_xgb.fit(train, train_labels)\n",
    "   \n",
    "    # Calculate predictions\n",
    "    preds = clf_xgb.predict_proba(test)\n",
    "    return preds\n",
    "\n",
    "print(\"Training Model1\")    \n",
    "preds_model1 = Model1(train, test, train_labels)         # compute predictions of Model1\n",
    "\n",
    "print(\"Training Model2\")\n",
    "preds_model2 = Model2(train, test, train_labels)         # compute predictions of Model2\n",
    "\n",
    "# compute final predictions for submission  \n",
    "preds_ensemble = (preds_model1**0.585) * (preds_model2**0.415)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81998021760633033"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds_model1.argmax(axis = 1) == test_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80019782393669636"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds_model2.argmax(axis = 1) == test_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82195845697329373"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds_ensemble.argmax(axis = 1) == test_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ensemble.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = preds_ensemble.argmax(axis = 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24, 1793,  368, ...,  368,  368,   24])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(nn_kmeans_train['author_id'])\n",
    "le.inverse_transform(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1011,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  24, 1793,  368, ...,  368,  368,   24])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_encoding(predicted, origin_train_label):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(origin_train_label)\n",
    "    return le.inverse_transform(predicted)\n",
    "\n",
    "predicted_label = reverse_encoding(predicted, nn_kmeans_train['author_id'])\n",
    "print predicted_label.shape\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_label = pd.DataFrame(predicted_label, columns=['predicted_author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>painting_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>predicted_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4707</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91028</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8130</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4561</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5870</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4229</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53006</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4346</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8208</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28824</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92017</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>90040</td>\n",
       "      <td>1793</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24383</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>89187</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>91239</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13548</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>79365</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13599</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7184</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53191</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7003</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>110327</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20239</td>\n",
       "      <td>368</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>151341</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>144137</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16337</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>258520</td>\n",
       "      <td>368</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8185</td>\n",
       "      <td>368</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>89149</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13549</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>85326</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>89165</td>\n",
       "      <td>1793</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>116149</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>52385</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>92023</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>218675</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>35916</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>87716</td>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>4289</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>138956</td>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>91156</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>91648</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>23430</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>89475</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>90806</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5902</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>260038</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>270347</td>\n",
       "      <td>368</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>28851</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>103408</td>\n",
       "      <td>24</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>5944</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>23164</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>258483</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>137422</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>89924</td>\n",
       "      <td>1793</td>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>5955</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>6022</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>99362</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>23440</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>24582</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      painting_id  author_id  predicted_author\n",
       "0            4707         24                24\n",
       "1           91028       1793              1793\n",
       "2            8130        368               368\n",
       "3            4561         24                24\n",
       "4            5870         24                24\n",
       "5            4229         24                24\n",
       "6           53006        368               368\n",
       "7            4346         24                24\n",
       "8            8208        368               368\n",
       "9           28824         24                24\n",
       "10          92017       1793              1793\n",
       "11          90040       1793                24\n",
       "12          24383         24                24\n",
       "13          89187       1793              1793\n",
       "14          91239       1793              1793\n",
       "15          13548        368               368\n",
       "16          79365       1793              1793\n",
       "17          13599        368               368\n",
       "18           7184         24                24\n",
       "19          53191        368               368\n",
       "20           7003         24                24\n",
       "21         110327         24                24\n",
       "22          20239        368                24\n",
       "23         151341        368               368\n",
       "24         144137        368               368\n",
       "25          16337        368               368\n",
       "26         258520        368                24\n",
       "27           8185        368                24\n",
       "28          89149       1793              1793\n",
       "29          13549        368               368\n",
       "...           ...        ...               ...\n",
       "981         85326         24                24\n",
       "982         89165       1793               368\n",
       "983        116149         24                24\n",
       "984         52385       1793              1793\n",
       "985         92023       1793              1793\n",
       "986        218675         24                24\n",
       "987         35916       1793              1793\n",
       "988         87716         24              1793\n",
       "989          4289         24                24\n",
       "990        138956         24              1793\n",
       "991         91156       1793              1793\n",
       "992         91648       1793              1793\n",
       "993         23430        368               368\n",
       "994         89475       1793              1793\n",
       "995         90806       1793              1793\n",
       "996          5902         24                24\n",
       "997        260038         24                24\n",
       "998        270347        368              1793\n",
       "999         28851         24                24\n",
       "1000       103408         24              1793\n",
       "1001         5944         24                24\n",
       "1002        23164        368               368\n",
       "1003       258483        368               368\n",
       "1004       137422         24                24\n",
       "1005        89924       1793              1793\n",
       "1006         5955         24                24\n",
       "1007         6022         24                24\n",
       "1008        99362        368               368\n",
       "1009        23440        368               368\n",
       "1010        24582         24                24\n",
       "\n",
       "[1011 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ensemble_table = pd.concat([nn_kmeans_test[['painting_id', 'author_id']].reset_index(drop=True), \\\n",
    "           pd.DataFrame(predicted_label)], axis=1, ignore_index=False)\n",
    "preds_ensemble_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_ensemble_table).to_csv('data/net1_ensemble_stacking_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 24      175\n",
       " 1793    165\n",
       " 368     160\n",
       " Name: author_id, dtype: int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(train).any().any(), train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=654, criterion='gini', max_depth=6, min_samples_split=2, \n",
    "                            min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=0.7, \n",
    "                            max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=4,\n",
    "                            random_state=101, verbose=0, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=0.7, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=5,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=654, n_jobs=4, oob_score=False, random_state=101,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74085064292779423"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nn_03', 0.0),\n",
       " ('nn_04', 0.0),\n",
       " ('nn_33', 0.0),\n",
       " ('nn_35', 0.0),\n",
       " ('nn_09', 0.0),\n",
       " ('nn_44', 0.0),\n",
       " ('nn_12', 0.0),\n",
       " ('nn_26', 3.4425906519345161e-07),\n",
       " ('nn_37', 6.9093654593991408e-05),\n",
       " ('nn_07', 0.00011837811541873045),\n",
       " ('nn_49', 0.00013032426135594206),\n",
       " ('kmeans_labels', 0.00016312072428500981),\n",
       " ('nn_32', 0.000210412271765006),\n",
       " ('nn_39', 0.00021945128175758919),\n",
       " ('nn_41', 0.00024077795476673493),\n",
       " ('nn_02', 0.00044631722373067987),\n",
       " ('nn_13', 0.0007029121089946154),\n",
       " ('pca_18', 0.00091170026419235895),\n",
       " ('nn_19', 0.00092458560901130147),\n",
       " ('nn_47', 0.00097574448488871454),\n",
       " ('nn_30', 0.0011446214834100114),\n",
       " ('pca_00', 0.0011877954669823629),\n",
       " ('pca_13', 0.0012910016495179942),\n",
       " ('pca_06', 0.0013111531655109329),\n",
       " ('nn_08', 0.001403082514400866),\n",
       " ('nn_28', 0.0014083582499042101),\n",
       " ('nn_40', 0.0014785280800322746),\n",
       " ('pca_07', 0.0016197545260500106),\n",
       " ('nn_25', 0.0017926814621866923),\n",
       " ('nn_36', 0.001842643824584243),\n",
       " ('pca_02', 0.0019233984653084341),\n",
       " ('pca_14', 0.0020311132578190541),\n",
       " ('nn_29', 0.0021044969193151218),\n",
       " ('nn_42', 0.0022268044827029251),\n",
       " ('nn_11', 0.0025054325007297798),\n",
       " ('pca_16', 0.0025129208825870409),\n",
       " ('pca_19', 0.0025146973342316074),\n",
       " ('pca_15', 0.002580454399460131),\n",
       " ('pca_05', 0.0026094058604534012),\n",
       " ('nn_27', 0.0026151145781311037),\n",
       " ('nn_31', 0.0027408365990694967),\n",
       " ('nn_14', 0.0028529940842758009),\n",
       " ('pca_01', 0.0034399497097083566),\n",
       " ('nn_38', 0.0036848100515963606),\n",
       " ('pca_17', 0.0039194232712568148),\n",
       " ('pca_10', 0.0039901697849978627),\n",
       " ('pca_09', 0.0043409644893786315),\n",
       " ('nn_24', 0.0051330581020897596),\n",
       " ('nn_21', 0.0058691252514302943),\n",
       " ('nn_23', 0.0060036553244809691),\n",
       " ('nn_50', 0.0063586074127836663),\n",
       " ('nn_34', 0.0065327534959658566),\n",
       " ('pca_11', 0.0067182746979713255),\n",
       " ('pca_03', 0.0071590441902255383),\n",
       " ('pca_12', 0.0076442082933157931),\n",
       " ('nn_01', 0.0076584110263861393),\n",
       " ('nn_16', 0.0088357348575118094),\n",
       " ('nn_20', 0.0088381839237559583),\n",
       " ('nn_43', 0.010311027209134229),\n",
       " ('nn_22', 0.011191053225357649),\n",
       " ('height_width_ratio', 0.011632130098651463),\n",
       " ('nn_45', 0.011982888890527005),\n",
       " ('pca_08', 0.013769050398324909),\n",
       " ('nn_15', 0.014171823527230057),\n",
       " ('pca_04', 0.026051669721398111),\n",
       " ('nn_05', 0.054608914653526866),\n",
       " ('nn_48', 0.059331244939731152),\n",
       " ('nn_46', 0.059885987504719941),\n",
       " ('nn_06', 0.0606693029029228),\n",
       " ('nn_18', 0.072636715396399279),\n",
       " ('nn_10', 0.19709407066507489),\n",
       " ('nn_17', 0.25172729498365704)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_imp = zip(train.columns, rf.feature_importances_)\n",
    "map(lambda x: ft_imp[x], rf.feature_importances_.argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3, 32, 34,  8, 43, 11, 25, 36,  6, 48, 71, 31, 38, 40,  1, 12,\n",
       "       69, 18, 46, 29, 51, 64, 57,  7, 27, 39, 58, 24, 35, 53, 65, 28, 41,\n",
       "       10, 67, 70, 66, 56, 26, 30, 13, 52, 37, 68, 61, 60, 23, 20, 22, 49,\n",
       "       33, 62, 54, 63,  0, 15, 19, 42, 21, 50, 44, 59, 14, 55,  4, 47, 45,\n",
       "        5, 17,  9, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(nthread = 4, learning_rate=.75, gamma=2, max_depth=7, min_child_weight=18,\n",
    "                          max_delta_step=0, subsample=0.75, colsample_bytree=0.6, silent=1, seed=2017,\n",
    "                          reg_lambda=1, n_estimators=450, objective='multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.6,\n",
       "       gamma=2, learning_rate=0.75, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=18, missing=None, n_estimators=450, nthread=4,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=2017, silent=1, subsample=0.75)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99158832261256802"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nn_26', 0.0),\n",
       " ('nn_03', 0.0),\n",
       " ('nn_04', 0.0),\n",
       " ('nn_49', 0.0),\n",
       " ('nn_44', 0.0),\n",
       " ('nn_09', 0.0),\n",
       " ('nn_31', 0.0),\n",
       " ('nn_12', 0.0),\n",
       " ('nn_35', 0.0),\n",
       " ('nn_33', 0.0),\n",
       " ('nn_41', 0.00070077082),\n",
       " ('nn_07', 0.0014015416),\n",
       " ('nn_39', 0.0014015416),\n",
       " ('nn_32', 0.0014015416),\n",
       " ('nn_37', 0.0021023126),\n",
       " ('nn_02', 0.0056061666),\n",
       " ('kmeans_labels', 0.0063069374),\n",
       " ('nn_27', 0.0070077083),\n",
       " ('nn_13', 0.0070077083),\n",
       " ('nn_40', 0.0070077083),\n",
       " ('nn_36', 0.0091100214),\n",
       " ('nn_19', 0.0091100214),\n",
       " ('nn_23', 0.0098107923),\n",
       " ('nn_34', 0.0098107923),\n",
       " ('nn_30', 0.010511563),\n",
       " ('nn_01', 0.011212333),\n",
       " ('nn_43', 0.011212333),\n",
       " ('nn_15', 0.011212333),\n",
       " ('nn_24', 0.012613875),\n",
       " ('nn_29', 0.012613875),\n",
       " ('nn_17', 0.012613875),\n",
       " ('nn_10', 0.012613875),\n",
       " ('nn_42', 0.013314646),\n",
       " ('nn_08', 0.013314646),\n",
       " ('nn_18', 0.013314646),\n",
       " ('nn_11', 0.014015417),\n",
       " ('nn_47', 0.014015417),\n",
       " ('nn_14', 0.014716187),\n",
       " ('pca_00', 0.014716187),\n",
       " ('nn_38', 0.015416958),\n",
       " ('nn_28', 0.015416958),\n",
       " ('nn_25', 0.015416958),\n",
       " ('pca_13', 0.016117729),\n",
       " ('nn_06', 0.016117729),\n",
       " ('pca_16', 0.016818501),\n",
       " ('pca_01', 0.016818501),\n",
       " ('nn_22', 0.017519271),\n",
       " ('pca_19', 0.018220043),\n",
       " ('nn_16', 0.018220043),\n",
       " ('pca_02', 0.018920813),\n",
       " ('pca_17', 0.018920813),\n",
       " ('nn_20', 0.019621585),\n",
       " ('nn_21', 0.019621585),\n",
       " ('nn_05', 0.020322355),\n",
       " ('nn_46', 0.020322355),\n",
       " ('nn_50', 0.021023126),\n",
       " ('pca_18', 0.021023126),\n",
       " ('nn_48', 0.021723896),\n",
       " ('nn_45', 0.023125438),\n",
       " ('pca_05', 0.023125438),\n",
       " ('pca_06', 0.023125438),\n",
       " ('pca_07', 0.023125438),\n",
       " ('pca_15', 0.023826208),\n",
       " ('pca_03', 0.02452698),\n",
       " ('pca_09', 0.02452698),\n",
       " ('pca_14', 0.025928522),\n",
       " ('pca_04', 0.025928522),\n",
       " ('pca_12', 0.028731605),\n",
       " ('pca_11', 0.030133147),\n",
       " ('pca_08', 0.033637002),\n",
       " ('pca_10', 0.036440086),\n",
       " ('height_width_ratio', 0.036440086)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_imp = zip(train.columns, xgb.feature_importances_)\n",
    "map(lambda x: ft_imp[x], xgb.feature_importances_.argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4042, 74)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pca_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4042, 74)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pca_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
